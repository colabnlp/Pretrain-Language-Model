{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5GMUBhNfZFW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hHz1jzC00vrQ",
    "outputId": "5c8f3a87-43dd-4156-b463-077f2c2f2909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: /opt/bin/nvidia-smi: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!/opt/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MDzTyqSDfsmu",
    "outputId": "ebd66397-ca5d-47d1-9652-a3e2d9a3a7bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "r41554Laf4uj",
    "outputId": "63b16513-433f-4f94-b7c7-f879b04c3f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running in local environment!\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "try:\n",
    "  from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "  gdd.download_file_from_google_drive(\n",
    "          file_id='1RFZmH6cLFbivA0VeDc6s56bRTgc2NhL1',\n",
    "          dest_path='NER_Data/vocab.txt',\n",
    "      )\n",
    "\n",
    "  gdd.download_file_from_google_drive(\n",
    "          file_id='1JqV332A6ZWZEHv64vzCFmSNVpC1Mox8J',\n",
    "          dest_path='NER_Data/msra.zip',\n",
    "          unzip=True\n",
    "      )\n",
    "  print('running in colab!')\n",
    "  MAIN_PATH = '/content/NER_Data/'\n",
    "except:\n",
    "  print('running in local environment!')\n",
    "  MAIN_PATH = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "NtzaciW3fZFc",
    "outputId": "7cc17c21-7526-44e8-f993-b186e7c793e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model params: {'target_size': 7}\n",
      "data params: {'seq_len': 200, 'batch_size': 64}\n",
      "train parmas: {'epochs': 10, 'lr': 0.001}\n",
      "tag2idx: {'O': 0, 'B-ORG': 1, 'I-ORG': 2, 'B-LOC': 3, 'I-LOC': 4, 'B-PER': 5, 'I-PER': 6}\n"
     ]
    }
   ],
   "source": [
    "# params for model\n",
    "model_config = dict(\n",
    "        target_size=7\n",
    "    )\n",
    "\n",
    "# params for data processing\n",
    "data_config = dict(\n",
    "        seq_len=200,\n",
    "        batch_size=64,\n",
    "    )\n",
    "\n",
    "# params for model training\n",
    "train_config = dict(\n",
    "        epochs=10,\n",
    "        lr=1e-3,\n",
    "    )\n",
    "\n",
    "tag2idx = {'O':0, \n",
    "           'B-ORG':1, 'I-ORG':2, \n",
    "           'B-LOC':3, 'I-LOC':4, \n",
    "           'B-PER':5, 'I-PER':6}\n",
    "\n",
    "train_pathx = MAIN_PATH+'msra/train/sentences.txt'\n",
    "train_pathy = MAIN_PATH+'msra/train/tags.txt'\n",
    "val_pathx = MAIN_PATH+'msra/val/sentences.txt'\n",
    "val_pathy = MAIN_PATH+'msra/val/tags.txt'\n",
    "test_pathx = MAIN_PATH+'msra/test/sentences.txt'\n",
    "test_pathy = MAIN_PATH+'msra/test/tags.txt'\n",
    "\n",
    "print('model params:', model_config)\n",
    "print('data params:', data_config)\n",
    "print('train parmas:', train_config)\n",
    "print('tag2idx:', tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fc32f0f26944a0b1d017c2a5657b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=109540.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f74be3aaf0403b8ff4da5576a26c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=568.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa54a41739864a6db415ee2353a1a380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411577189.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# bert fine-tuning for ner task\n",
    "\n",
    "# get bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "#model = BertForTokenClassification.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "CoZGwBDMfZFn",
    "outputId": "5d5a5fe8-3b18-4e12-9824-1f20b50589a6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['当', '有', '了', '一', '定', '的', '实', '力', '后', '，', '他', '就', '成', '立', '了', '武', '义', '县', '重', '点', '实', '用', '菌', '公', '司', '，', '不', '仅', '负', '责', '为', '菇', '农', '提', '供', '技', '术', '指', '导', '和', '菌', '种', '，', '而', '且', '负', '责', '原', '料', '代', '购', '，', '产', '品', '回', '收', '，', '经', '自', '己', '加', '工', '，', '或', '出', '口', '、', '或', '内', '销', '，', '从', '而', '使', '高', '温', '香', '菇', '栽', '培', '技', '术', '迅', '速', '扩', '散', '到', '浙', '西', '南', '山', '区', '的', '１', '０', '多', '个', '县', '市', '，', '１', '０', '０', '多', '个', '乡', '镇', '，', '栽', '培', '规', '模', '由', '１', '９', '９', '１', '年', '的', '２', '３', '万', '袋', '增', '加', '到', '１', '９', '９', '５', '年', '的', '３', '０', '０', '０', '万', '袋', '，', '仅', '此', '一', '项', '就', '使', '当', '地', '农', '民', '增', '加', '收', '入', '１', '亿', '多', '元', '。']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "10\n",
      "input_ids :\n",
      "[101, 1963, 862, 6237, 1104, 6639, 4413, 4518, 7270, 102]\n",
      "token_type_ids :\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask :\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokens (str)      : ['[CLS]', '如', '何', '解', '决', '足', '球', '界', '长', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# prepare train data\n",
    "train_token = open(train_pathx, 'r').read().split('\\n')[:-1]\n",
    "train_target = open(train_pathy, 'r').read().split('\\n')[:-1]\n",
    "\n",
    "print(tokenizer.tokenize(train_token[10]))\n",
    "print(train_target[10].split(' '))\n",
    "token_0 = tokenizer.encode_plus(train_token[0], max_length=10, pad_to_max_length=True)\n",
    "print(len(token_0['input_ids']))\n",
    "\n",
    "for key in token_0:\n",
    "    print(key, ':')\n",
    "    print(token_0[key])\n",
    "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in token_0['input_ids']]))\n",
    "#train_token = tokenizer.batch_encode_plus(train_token, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建数据集\n",
    "class MSRA(Dataset):\n",
    "    \n",
    "    def __init__(self, seq_len, train_token_path, train_target_path, device=device):\n",
    "        \n",
    "        # load raw data\n",
    "        train_token = open(train_token_path, 'r').read().split('\\n')[:-1]\n",
    "        train_target = open(train_target_path, 'r').read().split('\\n')[:-1]\n",
    "        \n",
    "        # tokenize\n",
    "        self.train_token = list(map(lambda x:['[CLS]']+x.split(' ')[:seq_len-2]+['[SEP]'], train_token))\n",
    "        self.train_target = list(map(lambda x:['O']+x.split(' ')[:seq_len-2]+['O'], train_target))\n",
    "        \n",
    "        # check\n",
    "        for token, target in zip(self.train_token, self.train_target):\n",
    "            if len(token) != len(target):\n",
    "                print(idx, token, target)\n",
    "                print('-'*100)\n",
    "        \n",
    "        # transform to id list\n",
    "        self.train_token = list(map(lambda x:tokenizer.convert_tokens_to_ids(x), self.train_token))\n",
    "        self.train_target = list(map(lambda x:[tag2idx[i] for i in x], self.train_target))\n",
    "        \n",
    "        # pad and mask\n",
    "        pad_lens = [seq_len-len(x) for x in self.train_token]\n",
    "        self.train_token = [token+[0]*pad_len for token, pad_len in zip(self.train_token, pad_lens)]\n",
    "        self.mask = [[1]*(seq_len-pad_len)+[0]*pad_len for pad_len in pad_lens]\n",
    "        self.train_target = [target+[0]*pad_len for target, pad_len in zip(self.train_target, pad_lens)]\n",
    "        \n",
    "        # to tensor\n",
    "        self.train_token = torch.tensor(self.train_token).to(device)\n",
    "        self.mask = torch.tensor(self.mask).to(device)\n",
    "        self.train_target = torch.tensor(self.train_target).to(device)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.train_token[idx], self.mask[idx], self.train_target[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_token)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 101, 2496, 3300,  749,  671, 2137, 4638, 2141, 1213, 1400, 8024,  800,\n",
      "        2218, 2768, 4989,  749, 3636,  721, 1344, 7028, 4157, 2141, 4500, 5826,\n",
      "        1062, 1385, 8024,  679,  788, 6566, 6569,  711, 5823, 1093, 2990,  897,\n",
      "        2825, 3318, 2900, 2193, 1469, 5826, 4905, 8024, 5445,  684, 6566, 6569,\n",
      "        1333, 3160,  807, 6579, 8024,  772, 1501, 1726, 3119, 8024, 5307, 5632,\n",
      "        2346, 1217, 2339, 8024, 2772, 1139, 1366,  510, 2772, 1079, 7218, 8024,\n",
      "         794, 5445,  886, 7770, 3946, 7676, 5823, 3420, 1824, 2825, 3318, 6813,\n",
      "        6862, 2810, 3141, 1168, 3851, 6205, 1298, 2255, 1277, 4638, 8029, 8028,\n",
      "        1914,  702, 1344, 2356, 8024, 8029, 8028, 8028, 1914,  702,  740, 7252,\n",
      "        8024, 3420, 1824, 6226, 3563, 4507, 8029, 8037, 8037, 8029, 2399, 4638,\n",
      "        8030, 8031,  674, 6150, 1872, 1217, 1168, 8029, 8037, 8037, 8033, 2399,\n",
      "        4638, 8031, 8028, 8028, 8028,  674, 6150, 8024,  788, 3634,  671, 7555,\n",
      "        2218,  886, 2496, 1765, 1093, 3696, 1872, 1217, 3119, 1057, 8029,  783,\n",
      "        1914, 1039,  511,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "train_data = MSRA(data_config['seq_len'], train_pathx, train_pathy)\n",
    "print(train_data[10])\n",
    "train_data_loader = DataLoader(train_data, batch_size=data_config['batch_size'])\n",
    "\n",
    "val_data = MSRA(data_config['seq_len'], val_pathx, val_pathy)\n",
    "val_data_loader = DataLoader(val_data, batch_size=data_config['batch_size'])\n",
    "\n",
    "test_data = MSRA(data_config['seq_len'], test_pathx, test_pathy)\n",
    "test_data_loader = DataLoader(test_data, batch_size=data_config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "-omtDVXMfZFq",
    "outputId": "761a35fb-c847-4209-dbde-40cba95b3349",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start...\n",
      "[params]:\n",
      "\tmodel params: {'target_size': 7}\n",
      "\ttrain parmas: {'epochs': 10, 'lr': 0.001}\n",
      "[build model]:\n",
      "<generator object Module.parameters at 0x1adcf1afc0>\n"
     ]
    }
   ],
   "source": [
    "# evaluate trained model on some cases\n",
    "def evaluation_case(model, path_x=MAIN_PATH+'msra/val/sentences.txt', path_y=MAIN_PATH+'msra/val/tags.txt', n=1):\n",
    "    Iter_val = train_data_iter(path_x, path_y, data_config, shuffle=False)\n",
    "    x,y = next(Iter_val)\n",
    "    tag_pred = model(x)\n",
    "    for sent, tag in zip(x[:n].numpy(), tag_pred[:n]):\n",
    "        print('\\t',[token_idx_r.get(i,'')+':'+tag_idx_r[j] for i,j in zip(sent, tag)])\n",
    "\n",
    "# train model and evaluation\n",
    "def train(model_config, train_config, device=device):\n",
    "    \n",
    "    print('training start...')\n",
    "    print('[params]:')\n",
    "    print('\\tmodel params:', model_config)\n",
    "    print('\\ttrain parmas:', train_config)\n",
    "\n",
    "    # build model\n",
    "    bertmodel = BertForTokenClassification.from_pretrained('bert-base-chinese', num_labels=model_config['target_size']).to(device)\n",
    "    print('[build model]:')\n",
    "    print(bertmodel.parameters())\n",
    "    \n",
    "    # opt\n",
    "    optimizer = AdamW(bertmodel.parameters(), lr=train_config['lr'])\n",
    "    \n",
    "    # train\n",
    "    for epoch in range(train_config['epochs']):\n",
    "        \n",
    "        # record loss every epoch\n",
    "        loss_value = []\n",
    "        \n",
    "        for token, mask, target in train_data_loader:\n",
    "\n",
    "            # model init\n",
    "            optimizer.zero_grad()\n",
    "            bertmodel.train()\n",
    "            \n",
    "            loss = bertmodel(input_ids=token, attention_mask=mask, labels=target)\n",
    "            \n",
    "            # record\n",
    "            loss_value.append(loss.item())\n",
    "            \n",
    "            # weight update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print('[epoch %d]\\tloss=%s' % (epoch, np.mean(loss_value)))\n",
    "        #print('all loss:', loss_value)\n",
    "        print('[evaluation]:')\n",
    "        #evaluation_case(model, n=2)\n",
    "        \n",
    "    return model\n",
    "\n",
    "model = train(model_config, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca5360f70684362a40d7d648fb3f0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "def test(model, out_pathx, out_pathy, path_x=MAIN_PATH+'msra/test/sentences.txt', path_y=MAIN_PATH+'msra/test/tags.txt'):\n",
    "    Iter_val = train_data_iter(path_x, path_y, data_config, shuffle=False)\n",
    "    ox = open(out_pathx, 'w')\n",
    "    oy = open(out_pathy, 'w')\n",
    "    \n",
    "    model.eval()\n",
    "    for x,y in tqdm_notebook(Iter_val):\n",
    "        with torch.no_grad():\n",
    "            tag_pred = model(x)\n",
    "            for sent, tag in zip(x.numpy(), tag_pred):\n",
    "                sent_decode = ' '.join([token_idx_r[i] for i in sent])\n",
    "                tag_decode = ' '.join([tag_idx_r[i] for i in tag])\n",
    "                ox.write(sent_decode + '\\n')\n",
    "                oy.write(tag_decode + '\\n')\n",
    "    ox.close()\n",
    "    oy.close()\n",
    "    \n",
    "test(model, out_pathx='result/lstm_crf_sentences.txt', out_pathy='result/lstm_crf_tags.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro : 0.8097027292120667 \t macro : 0.8097027292120667\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "from utils.evaluation import f1_score_from_path\n",
    "\n",
    "test_x = 'msra/test/sentences.txt'\n",
    "test_y = 'msra/test/tags.txt'\n",
    "pred_y = 'result/lstm_crf_tags.txt'\n",
    "pred_x = 'result/lstm_crf_sentences.txt' # Because of padding, the length of prediction may be shorter than true label\n",
    "\n",
    "micro_score = f1_score_from_path(test_x, test_y, pred_y, pred_x, f1_type='mirco')\n",
    "macro_score = f1_score_from_path(test_x, test_y, pred_y, pred_x, f1_type='marco')\n",
    "print('micro : %s \\t macro : %s' % (micro_score, macro_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "lstm_crf_baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
